{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"E:\\\\2_Studium\\\\1_Mannheim\\\\4_Semester\\\\Masterarbeit\\\\code\\\\master-thesis-code\\\\data\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_abt_buy_test = pd.read_csv(\"abt_buy/features_abt_buy_test\")\n",
    "feature_abt_buy_train = pd.read_csv(\"abt_buy/features_abt_buy_train\")\n",
    "abt_buy_gold_standard_train = pd.read_csv(\"abt_buy/abt_buy_gold_standard_train.csv\",sep=\";\")\n",
    "abt_buy_gold_standard_test = pd.read_csv(\"abt_buy/abt_buy_gold_standard_test.csv\",sep=\";\")\n",
    "\n",
    "feature_amazon_google_test = pd.read_csv(\"amazon_google/features_amazon_google_test\")\n",
    "feature_amazon_google_train = pd.read_csv(\"amazon_google/features_amazon_google_train\")\n",
    "gs_amazon_google_train = pd.read_csv(\"amazon_google/amazon_google_gold_standard_train.csv\",sep=\";\")\n",
    "gs_amazon_google_test = pd.read_csv(\"amazon_google/amazon_google_gold_standard_test.csv\",sep=\";\")\n",
    "\n",
    "feature_wdc_phones_test = pd.read_csv(\"wdc_product/features_phones_phones_catalog_test\")\n",
    "feature_wdc_phones_train = pd.read_csv(\"wdc_product/features_phones_phones_catalog_train\")\n",
    "gs_wdc_phones_train = pd.read_csv(\"wdc_product/phones_phones_catalog_gold_standard_train.csv\",sep=\";\")\n",
    "gs_wdc_phones_test = pd.read_csv(\"wdc_product/phones_phones_catalog_gold_standard_test.csv\",sep=\";\")\n",
    "\n",
    "\n",
    "feature_wdc_headphones_test = pd.read_csv(\"wdc_product/features_headphones_headphones_catalog_test\")\n",
    "feature_wdc_headphones_train = pd.read_csv(\"wdc_product/features_headphones_headphones_catalog_train\")\n",
    "gs_wdc_headphones_train = pd.read_csv(\"wdc_product/headphones_headphones_catalog_silver_standard_train.csv\",sep=\";\")\n",
    "gs_wdc_headphones_test = pd.read_csv(\"wdc_product/headphones_headphones_catalog_silver_standard_test.csv\",sep=\";\")\n",
    "\n",
    "feature_dbpedia_viaf_test = pd.read_csv(\"author/features_DBPediaAuthors_VIAFDataAuthors_test\")\n",
    "feature_dbpedia_viaf_train = pd.read_csv(\"author/features_DBPediaAuthors_VIAFDataAuthors_train\")\n",
    "gs_dbpedia_viaf_test = pd.read_csv(\"author/DBPediaAuthors_VIAFDataAuthors_gold_standard_test.csv\",sep=\";\")\n",
    "gs_dbpedia_viaf_train = pd.read_csv(\"author/DBPediaAuthors_VIAFDataAuthors_gold_standard_train.csv\",sep=\";\")\n",
    "\n",
    "feature_dbpedia_dnb_test = pd.read_csv(\"author/features_DBPediaAuthors_DnbDataAuthors_test\")\n",
    "feature_dbpedia_dnb_train = pd.read_csv(\"author/features_DBPediaAuthors_DnbDataAuthors_train\")\n",
    "gs_dbpedia_dnb_test = pd.read_csv(\"author/DBPediaAuthors_DnbDataAuthors_gold_standard_test.csv\",sep=\";\")\n",
    "gs_dbpedia_dnb_train = pd.read_csv(\"author/DBPediaAuthors_DnbDataAuthors_gold_standard_train.csv\",sep=\";\")\n",
    "\n",
    "\n",
    "\n",
    "#feature_dbpedia_wiki_test = pd.read_csv(\"author/features_DBPediaAuthors_WikiDataAuthors_test\")\n",
    "#feature_dbpedia_wiki_train = pd.read_csv(\"author/features_DBPediaAuthors_WikiDataAuthors_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Dataset  # Attributes  # Train  # Test\n",
       "0         abt_buy            19     5730    1432\n",
       "1   amazon_google            24     6753    1687\n",
       "2      wdc_phones           135     1762     440\n",
       "3  wdc_headphones           142     1163     290\n",
       "4    dbpedia_viaf            27    15316    3828\n",
       "5     dbpedia_dnb            26    13863    3465"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th># Attributes</th>\n      <th># Train</th>\n      <th># Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abt_buy</td>\n      <td>19</td>\n      <td>5730</td>\n      <td>1432</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>amazon_google</td>\n      <td>24</td>\n      <td>6753</td>\n      <td>1687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wdc_phones</td>\n      <td>135</td>\n      <td>1762</td>\n      <td>440</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wdc_headphones</td>\n      <td>142</td>\n      <td>1163</td>\n      <td>290</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dbpedia_viaf</td>\n      <td>27</td>\n      <td>15316</td>\n      <td>3828</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>dbpedia_dnb</td>\n      <td>26</td>\n      <td>13863</td>\n      <td>3465</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# Data profiling information\n",
    "metadata = [\n",
    "    ['abt_buy',feature_abt_buy_train.shape[1],\n",
    "    feature_abt_buy_train.shape[0],feature_abt_buy_test.shape[0]],\n",
    "    ['amazon_google',feature_amazon_google_train.shape[1],\n",
    "    feature_amazon_google_train.shape[0],feature_amazon_google_test.shape[0]],\n",
    "    ['wdc_phones',feature_wdc_phones_train.shape[1],\n",
    "    feature_wdc_phones_train.shape[0],feature_wdc_phones_test.shape[0]],\n",
    "    ['wdc_headphones',feature_wdc_headphones_train.shape[1],\n",
    "    feature_wdc_headphones_train.shape[0],feature_wdc_headphones_test.shape[0]],\n",
    "    ['dbpedia_viaf',feature_dbpedia_viaf_train.shape[1],\n",
    "    feature_dbpedia_viaf_train.shape[0],feature_dbpedia_viaf_test.shape[0]],\n",
    "    #['dbpedia_wiki',feature_dbpedia_wiki_train.shape[1],\n",
    "    #feature_dbpedia_wiki_train.shape[0],feature_dbpedia_wiki_test.shape[0]],\n",
    "    ['dbpedia_dnb',feature_dbpedia_dnb_train.shape[1],\n",
    "    feature_dbpedia_dnb_train.shape[0],feature_dbpedia_dnb_test.shape[0]],\n",
    "    ]\n",
    "df_profiling = pd.DataFrame(metadata, columns=['Dataset', '# Attributes', '# Train', '# Test'])\n",
    "df_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing abt_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_abt_buy_train = feature_abt_buy_train[[\"source_id\",\"target_id\"]]\n",
    "ids_abt_buy_test = feature_abt_buy_test[[\"source_id\",\"target_id\"]]\n",
    "abt_original = pd.read_csv(\"abt_buy/abt.csv\",encoding='latin1').add_prefix('left_')\n",
    "buy_original = pd.read_csv(\"abt_buy/buy.csv\",encoding='latin1').add_prefix('right_')\n",
    "\n",
    "#train\n",
    "abt_buy_train = ids_abt_buy_train.merge(\n",
    "    abt_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    buy_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "abt_buy_train['id'] = abt_buy_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#test\n",
    "abt_buy_test = ids_abt_buy_test.merge(\n",
    "    abt_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    buy_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "abt_buy_test['id'] = abt_buy_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "abt_buy_gold_standard_train['id'] = abt_buy_gold_standard_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "abt_buy_gold_standard_train.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "#test\n",
    "abt_buy_gold_standard_test['id'] = abt_buy_gold_standard_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "abt_buy_gold_standard_test.drop(['source_id','target_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_buy_train = abt_buy_train.merge(abt_buy_gold_standard_train,on='id')\n",
    "abt_buy_train = abt_buy_train.rename(columns={'matching':'label'})\n",
    "abt_buy_train.drop('right_manufacturer',axis=1,inplace=True)\n",
    "abt_buy_train['label'] = abt_buy_train['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   source_id  target_id                                          left_name  \\\n",
       "0      17187  202041204                     Universal IR/RF Remote - MX350   \n",
       "1      17187  203069939                     Universal IR/RF Remote - MX350   \n",
       "2      33965  203069939  Logitech Harmony One Advanced Universal Remote...   \n",
       "3      31273  203069939  Sony Universal Remote Commander Remote Control...   \n",
       "4      27779  205844283  Sony LCS-CSQ/B Black Soft Carrying Case - LCSCSQB   \n",
       "\n",
       "                                    left_description  left_price  \\\n",
       "0  Universal IR/RF Remote - MX350/ Controls Up To...      149.95   \n",
       "1  Universal IR/RF Remote - MX350/ Controls Up To...      149.95   \n",
       "2  Logitech Harmony One Advanced Universal Remote...      249.00   \n",
       "3  Sony Universal Remote Commander Remote Control...         NaN   \n",
       "4  Sony LCS-CSQ/B Black Soft Carrying Case - LCSC...         NaN   \n",
       "\n",
       "                                          right_name  \\\n",
       "0     Universal MX-350 Osiris Remote Control - MX350   \n",
       "1  Universal MasterControl Universal Learning Rem...   \n",
       "2  Universal MasterControl Universal Learning Rem...   \n",
       "3  Universal MasterControl Universal Learning Rem...   \n",
       "4  Sony LCS-CSQ Soft Cyber-shot Camera Case - LCS...   \n",
       "\n",
       "                                   right_description  right_price  \\\n",
       "0  TV, VCR, DVD Player, CD Player, Cable Box, Sat...         93.9   \n",
       "1  Audio/Video, TV, VCR, DVD Player, CD Player, C...          NaN   \n",
       "2  Audio/Video, TV, VCR, DVD Player, CD Player, C...          NaN   \n",
       "3  Audio/Video, TV, VCR, DVD Player, CD Player, C...          NaN   \n",
       "4                    Top Loading - Polyamide - Black          NaN   \n",
       "\n",
       "                id  label  \n",
       "0  17187-202041204      1  \n",
       "1  17187-203069939      0  \n",
       "2  33965-203069939      0  \n",
       "3  31273-203069939      0  \n",
       "4  27779-205844283      1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source_id</th>\n      <th>target_id</th>\n      <th>left_name</th>\n      <th>left_description</th>\n      <th>left_price</th>\n      <th>right_name</th>\n      <th>right_description</th>\n      <th>right_price</th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17187</td>\n      <td>202041204</td>\n      <td>Universal IR/RF Remote - MX350</td>\n      <td>Universal IR/RF Remote - MX350/ Controls Up To...</td>\n      <td>149.95</td>\n      <td>Universal MX-350 Osiris Remote Control - MX350</td>\n      <td>TV, VCR, DVD Player, CD Player, Cable Box, Sat...</td>\n      <td>93.9</td>\n      <td>17187-202041204</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17187</td>\n      <td>203069939</td>\n      <td>Universal IR/RF Remote - MX350</td>\n      <td>Universal IR/RF Remote - MX350/ Controls Up To...</td>\n      <td>149.95</td>\n      <td>Universal MasterControl Universal Learning Rem...</td>\n      <td>Audio/Video, TV, VCR, DVD Player, CD Player, C...</td>\n      <td>NaN</td>\n      <td>17187-203069939</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>33965</td>\n      <td>203069939</td>\n      <td>Logitech Harmony One Advanced Universal Remote...</td>\n      <td>Logitech Harmony One Advanced Universal Remote...</td>\n      <td>249.00</td>\n      <td>Universal MasterControl Universal Learning Rem...</td>\n      <td>Audio/Video, TV, VCR, DVD Player, CD Player, C...</td>\n      <td>NaN</td>\n      <td>33965-203069939</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31273</td>\n      <td>203069939</td>\n      <td>Sony Universal Remote Commander Remote Control...</td>\n      <td>Sony Universal Remote Commander Remote Control...</td>\n      <td>NaN</td>\n      <td>Universal MasterControl Universal Learning Rem...</td>\n      <td>Audio/Video, TV, VCR, DVD Player, CD Player, C...</td>\n      <td>NaN</td>\n      <td>31273-203069939</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>27779</td>\n      <td>205844283</td>\n      <td>Sony LCS-CSQ/B Black Soft Carrying Case - LCSCSQB</td>\n      <td>Sony LCS-CSQ/B Black Soft Carrying Case - LCSC...</td>\n      <td>NaN</td>\n      <td>Sony LCS-CSQ Soft Cyber-shot Camera Case - LCS...</td>\n      <td>Top Loading - Polyamide - Black</td>\n      <td>NaN</td>\n      <td>27779-205844283</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "abt_buy_test = abt_buy_test.merge(abt_buy_gold_standard_test,on='id')\n",
    "abt_buy_test = abt_buy_test.rename(columns={'matching':'label'})\n",
    "abt_buy_test.drop('right_manufacturer',axis=1,inplace=True)\n",
    "abt_buy_test['label'] = abt_buy_test['label'].astype(int)\n",
    "abt_buy_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = abt_buy_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(abt_buy_train, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_buy_train = X_train\n",
    "abt_buy_validation = X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "abt_buy_train.to_csv(\"abt_buy/abt_buy_train\",index=False)\n",
    "abt_buy_validation.to_csv(\"abt_buy/abt_buy_validation\",index=False)\n",
    "abt_buy_test.to_csv(\"abt_buy/abt_buy_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(abt_buy_train['label'] == 1))\n",
    "print(sum(abt_buy_train['label'] == 0))\n",
    "print(sum(abt_buy_validation['label'] == 1))\n",
    "print(sum(abt_buy_validation['label'] == 0))\n",
    "print(sum(abt_buy_test['label'] == 1))\n",
    "print(sum(abt_buy_test['label'] == 0))"
   ]
  },
  {
   "source": [
    "# Preprocessing dbpedia_viaf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dbpedia_viaf_train = feature_dbpedia_viaf_train[[\"source_id\",\"target_id\"]]\n",
    "ids_dbpedia_viaf_test = feature_dbpedia_viaf_test[[\"source_id\",\"target_id\"]]\n",
    "dbpedia_original = pd.read_csv(\"author/DBpediaAuthors.csv\",encoding='latin1').add_prefix('left_')\n",
    "viaf_original = pd.read_csv(\"author/VIAFDataAuthors.csv\",encoding='latin1').add_prefix('right_')\n",
    "\n",
    "#train\n",
    "dbpedia_viaf_train = ids_dbpedia_viaf_train.merge(\n",
    "    dbpedia_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    viaf_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "dbpedia_viaf_train['id'] = dbpedia_viaf_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#test\n",
    "dbpedia_viaf_test = ids_dbpedia_viaf_test.merge(\n",
    "    dbpedia_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    viaf_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "dbpedia_viaf_test['id'] = dbpedia_viaf_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "gs_dbpedia_viaf_train['id'] = gs_dbpedia_viaf_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_dbpedia_viaf_train.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "#test\n",
    "gs_dbpedia_viaf_test['id'] = gs_dbpedia_viaf_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_dbpedia_viaf_test.drop(['source_id','target_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_viaf_train = dbpedia_viaf_train.merge(gs_dbpedia_viaf_train,on='id')\n",
    "dbpedia_viaf_train = dbpedia_viaf_train.rename(columns={'matching':'label'})\n",
    "dbpedia_viaf_train.drop(['left_abstract','left_links'],axis=1,inplace=True)\n",
    "dbpedia_viaf_train['label'] = dbpedia_viaf_train['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_viaf_test = dbpedia_viaf_test.merge(gs_dbpedia_viaf_test,on='id')\n",
    "dbpedia_viaf_test = dbpedia_viaf_test.rename(columns={'matching':'label'})\n",
    "dbpedia_viaf_test.drop(['left_abstract','left_links'],axis=1,inplace=True)\n",
    "dbpedia_viaf_test['label'] = dbpedia_viaf_test['label'].astype(int)"
   ]
  },
  {
   "source": [
    "## Split Train into Train and Validation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dbpedia_viaf_train['label']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(dbpedia_viaf_train, y,\n",
    "                                                stratify=y, \n",
    "                                                test_size=0.25)\n",
    "dbpedia_viaf_train = X_train\n",
    "dbpedia_viaf_validation = X_val\n",
    "dbpedia_viaf_train.to_csv(\"author/dbpedia_viaf_train\",index=False)\n",
    "dbpedia_viaf_validation.to_csv(\"author/dbpedia_viaf_validation\",index=False)\n",
    "dbpedia_viaf_test.to_csv(\"author/dbpedia_viaf_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        Dataset  # Train  # Validation  # Test\n",
       "0  dbpedia_viaf    10885          3629    3651"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Dataset</th>\n      <th># Train</th>\n      <th># Validation</th>\n      <th># Test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dbpedia_viaf</td>\n      <td>10885</td>\n      <td>3629</td>\n      <td>3651</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "# Data profiling information blocked pairs\n",
    "metadata_2 = [['dbpedia_viaf',dbpedia_viaf_train.shape[0],dbpedia_viaf_validation.shape[0],dbpedia_viaf_test.shape[0]]]\n",
    "df_blocking = pd.DataFrame(metadata_2, columns=['Dataset', '# Train', '# Validation', '# Test'])\n",
    "df_blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1759\n9126\n587\n3042\n591\n3060\n"
     ]
    }
   ],
   "source": [
    "print(sum(dbpedia_viaf_train['label'] == 1))\n",
    "print(sum(dbpedia_viaf_train['label'] == 0))\n",
    "print(sum(dbpedia_viaf_validation['label'] == 1))\n",
    "print(sum(dbpedia_viaf_validation['label'] == 0))\n",
    "print(sum(dbpedia_viaf_test['label'] == 1))\n",
    "print(sum(dbpedia_viaf_test['label'] == 0))"
   ]
  },
  {
   "source": [
    "# Preprocessing dbpedia_dnb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_dbpedia_dnb_train = feature_dbpedia_dnb_train[[\"source_id\",\"target_id\"]]\n",
    "ids_dbpedia_dnb_test = feature_dbpedia_dnb_test[[\"source_id\",\"target_id\"]]\n",
    "dbpedia_original = pd.read_csv(\"author/DBpediaAuthors.csv\",encoding='latin1').add_prefix('left_')\n",
    "dnb_original = pd.read_csv(\"author/DnbDataAuthors.csv\",encoding='latin1').add_prefix('right_')\n",
    "\n",
    "#train\n",
    "dbpedia_dnb_train = ids_dbpedia_dnb_train.merge(\n",
    "    dbpedia_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    dnb_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "dbpedia_dnb_train['id'] = dbpedia_dnb_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#test\n",
    "dbpedia_dnb_test = ids_dbpedia_dnb_test.merge(\n",
    "    dbpedia_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    dnb_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "dbpedia_dnb_test['id'] = dbpedia_dnb_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#train\n",
    "gs_dbpedia_dnb_train['id'] = gs_dbpedia_dnb_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_dbpedia_dnb_train.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "#test\n",
    "gs_dbpedia_dnb_test['id'] = gs_dbpedia_dnb_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_dbpedia_dnb_test.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "dbpedia_dnb_train = dbpedia_dnb_train.merge(gs_dbpedia_dnb_train,on='id')\n",
    "dbpedia_dnb_train = dbpedia_dnb_train.rename(columns={'matching':'label'})\n",
    "dbpedia_dnb_train.drop(\n",
    "    ['left_abstract','left_links','left_work',\n",
    "    'right_forename','right_nameAddition','right_personalName',\n",
    "    'right_prefix',\n",
    "    'right_surname',\n",
    "    'right_work'],axis=1,inplace=True)\n",
    "dbpedia_dnb_train['label'] = dbpedia_dnb_train['label'].astype(int)\n",
    "\n",
    "dbpedia_dnb_test = dbpedia_dnb_test.merge(gs_dbpedia_dnb_test,on='id')\n",
    "dbpedia_dnb_test = dbpedia_dnb_test.rename(columns={'matching':'label'})\n",
    "dbpedia_dnb_test.drop(\n",
    "    ['left_abstract','left_links','left_work',\n",
    "    'right_forename','right_nameAddition','right_personalName',\n",
    "    'right_prefix',\n",
    "    'right_surname',\n",
    "    'right_work'],axis=1,inplace=True)\n",
    "dbpedia_dnb_test['label'] = dbpedia_dnb_test['label'].astype(int)\n",
    "\n",
    "y = dbpedia_dnb_train['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(dbpedia_dnb_train, y, stratify=y, test_size=0.25)\n",
    "\n",
    "# Split Train into Train and Validation\n",
    "dbpedia_dnb_train = X_train\n",
    "dbpedia_dnb_validation = X_val\n",
    "dbpedia_dnb_train.to_csv(\"author/dbpedia_dnb_train\",index=False)\n",
    "dbpedia_dnb_validation.to_csv(\"author/dbpedia_dnb_validation\",index=False)\n",
    "dbpedia_dnb_test.to_csv(\"author/dbpedia_dnb_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1401\n7418\n467\n2473\n469\n2493\n"
     ]
    }
   ],
   "source": [
    "print(sum(dbpedia_dnb_train['label'] == 1))\n",
    "print(sum(dbpedia_dnb_train['label'] == 0))\n",
    "print(sum(dbpedia_dnb_validation['label'] == 1))\n",
    "print(sum(dbpedia_dnb_validation['label'] == 0))\n",
    "print(sum(dbpedia_dnb_test['label'] == 1))\n",
    "print(sum(dbpedia_dnb_test['label'] == 0))"
   ]
  },
  {
   "source": [
    "# Preprocessing amazon_google"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_amazon_google_train = feature_amazon_google_train[[\"source_id\",\"target_id\"]]\n",
    "ids_amazon_google_test = feature_amazon_google_test[[\"source_id\",\"target_id\"]]\n",
    "amazon_original = pd.read_csv(\"amazon_google/amazon.csv\",encoding='latin1').add_prefix('left_')\n",
    "google_original = pd.read_csv(\"amazon_google/google.csv\",encoding='latin1').add_prefix('right_')\n",
    "\n",
    "#train\n",
    "amazon_google_train = ids_amazon_google_train.merge(\n",
    "    amazon_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    google_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "amazon_google_train['id'] = amazon_google_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#test\n",
    "amazon_google_test = ids_amazon_google_test.merge(\n",
    "    amazon_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    google_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "amazon_google_test['id'] = amazon_google_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#train\n",
    "gs_amazon_google_train['id'] = gs_amazon_google_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_amazon_google_train.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "#test\n",
    "gs_amazon_google_test['id'] = gs_amazon_google_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_amazon_google_test.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "amazon_google_train = amazon_google_train.merge(gs_amazon_google_train,on='id')\n",
    "amazon_google_train = amazon_google_train.rename(columns={'matching':'label'})\n",
    "amazon_google_train['label'] = amazon_google_train['label'].astype(int)\n",
    "\n",
    "amazon_google_test = amazon_google_test.merge(gs_amazon_google_test,on='id')\n",
    "amazon_google_test = amazon_google_test.rename(columns={'matching':'label'})\n",
    "amazon_google_test['label'] = amazon_google_test['label'].astype(int)\n",
    "\n",
    "y = amazon_google_train['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(amazon_google_train, y, stratify=y, test_size=0.25)\n",
    "\n",
    "# Split Train into Train and Validation\n",
    "amazon_google_train = X_train\n",
    "amazon_google_validation = X_val\n",
    "\n",
    "# workaround to truncate columns\n",
    "amazon_google_train['left_description'] = amazon_google_train['left_description'].apply(lambda x: x if isinstance(x, float) else x[:500])\n",
    "amazon_google_validation['left_description'] = amazon_google_validation['left_description'].apply(lambda x: x if isinstance(x, float) else x[:500])\n",
    "amazon_google_test['left_description'] = amazon_google_test['left_description'].apply(lambda x: x if isinstance(x, float) else x[:500])\n",
    "\n",
    "amazon_google_train.to_csv(\"amazon_google/amazon_google_train\",index=False)\n",
    "amazon_google_validation.to_csv(\"amazon_google/amazon_google_validation\",index=False)\n",
    "amazon_google_test.to_csv(\"amazon_google/amazon_google_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "779\n4285\n260\n1429\n259\n1428\n"
     ]
    }
   ],
   "source": [
    "print(sum(amazon_google_train['label'] == 1))\n",
    "print(sum(amazon_google_train['label'] == 0))\n",
    "print(sum(amazon_google_validation['label'] == 1))\n",
    "print(sum(amazon_google_validation['label'] == 0))\n",
    "print(sum(amazon_google_test['label'] == 1))\n",
    "print(sum(amazon_google_test['label'] == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'source_id': 10,\n",
       " 'target_id': 62,\n",
       " 'left_name': 109,\n",
       " 'left_description': 19263,\n",
       " 'left_manufacturer': 45,\n",
       " 'left_price': 9,\n",
       " 'right_name': 223,\n",
       " 'right_description': 253,\n",
       " 'right_manufacturer': 31,\n",
       " 'right_price': 8,\n",
       " 'id': 73,\n",
       " 'label': 1}"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "df = pd.read_csv(\"amazon_google/amazon_google_train\")\n",
    "#df = pd.read_csv(\"abt_buy/abt_buy_train\")\n",
    "dict([(v, df[v].apply(lambda r: len(str(r)) if r!=None else 0).max())for v in df.columns.values])"
   ]
  },
  {
   "source": [
    "# Preprocessing wdc_phones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\nipykernel_launcher:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_wdc_phones_train = feature_wdc_phones_train[[\"source_id\",\"target_id\"]]\n",
    "ids_wdc_phones_test = feature_wdc_phones_test[[\"source_id\",\"target_id\"]]\n",
    "phones_original = pd.read_csv(\"wdc_product/phones.csv\",sep='\\|\\|').drop('Unnamed: 29',axis=1)\n",
    "phones_original = phones_original.rename(columns=lambda x: x.rsplit('/',1)[1][:-1] if x.endswith('>') else x).add_prefix('left_')\n",
    "phones_catalog_original = pd.read_csv(\"wdc_product/phones_catalog.csv\",sep='\\|\\|').drop('Unnamed: 31',axis=1)\n",
    "phones_catalog_original = phones_catalog_original.rename(columns=lambda x: x.rsplit('/',1)[1][:-1] if x.endswith('>') else x).add_prefix('right_')\n",
    "\n",
    "#train\n",
    "wdc_phones_train = ids_wdc_phones_train.merge(\n",
    "    phones_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    phones_catalog_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "wdc_phones_train['id'] = wdc_phones_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#test\n",
    "wdc_phones_test = ids_wdc_phones_test.merge(\n",
    "    phones_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    phones_catalog_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "wdc_phones_test['id'] = wdc_phones_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#train\n",
    "gs_wdc_phones_train['id'] = gs_wdc_phones_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_wdc_phones_train.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "#test\n",
    "gs_wdc_phones_test['id'] = gs_wdc_phones_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_wdc_phones_test.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "wdc_phones_train = wdc_phones_train.merge(gs_wdc_phones_train,on='id')\n",
    "wdc_phones_train = wdc_phones_train.rename(columns={'matching':'label'})\n",
    "wdc_phones_train.drop(\n",
    "    ['left_warc',\n",
    "    'left_url',\n",
    "    'right_voltage',\n",
    "    'right_package_height',\n",
    "    'right_core_count',\n",
    "    'right_power_supply'],axis=1,inplace=True)\n",
    "wdc_phones_train['label'] = wdc_phones_train['label'].astype(int)\n",
    "\n",
    "wdc_phones_test = wdc_phones_test.merge(gs_wdc_phones_test,on='id')\n",
    "wdc_phones_test = wdc_phones_test.rename(columns={'matching':'label'})\n",
    "wdc_phones_test.drop(\n",
    "    ['left_warc',\n",
    "    'left_url',\n",
    "    'right_voltage',\n",
    "    'right_package_height',\n",
    "    'right_core_count',\n",
    "    'right_power_supply'],axis=1,inplace=True)\n",
    "wdc_phones_test['label'] = wdc_phones_test['label'].astype(int)\n",
    "\n",
    "y = wdc_phones_train['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(wdc_phones_train, y, stratify=y, test_size=0.25)\n",
    "\n",
    "# Split Train into Train and Validation\n",
    "wdc_phones_train = X_train\n",
    "wdc_phones_validation = X_val\n",
    "wdc_phones_train.to_csv(\"wdc_product/wdc_phones_train\",index=False)\n",
    "wdc_phones_validation.to_csv(\"wdc_product/wdc_phones_validation\",index=False)\n",
    "wdc_phones_test.to_csv(\"wdc_product/wdc_phones_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "154\n1167\n52\n389\n51\n389\n"
     ]
    }
   ],
   "source": [
    "print(sum(wdc_phones_train['label'] == 1))\n",
    "print(sum(wdc_phones_train['label'] == 0))\n",
    "print(sum(wdc_phones_validation['label'] == 1))\n",
    "print(sum(wdc_phones_validation['label'] == 0))\n",
    "print(sum(wdc_phones_test['label'] == 1))\n",
    "print(sum(wdc_phones_test['label'] == 0))"
   ]
  },
  {
   "source": [
    "# Preprocessing wdc_headphones"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\nipykernel_launcher:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ids_wdc_headphones_train = feature_wdc_headphones_train[[\"source_id\",\"target_id\"]]\n",
    "ids_wdc_headphones_test = feature_wdc_headphones_test[[\"source_id\",\"target_id\"]]\n",
    "headphones_original = pd.read_csv(\"wdc_product/headphones.csv\",sep='\\|\\|').drop('Unnamed: 30',axis=1)\n",
    "headphones_original = headphones_original.rename(columns=lambda x: x.rsplit('/',1)[1][:-1] if x.endswith('>') else x).add_prefix('left_')\n",
    "headphones_catalog_original = pd.read_csv(\"wdc_product/headphones_catalog.csv\",sep='\\|\\|').drop('Unnamed: 39',axis=1)\n",
    "headphones_catalog_original = headphones_catalog_original.rename(columns=lambda x: x.rsplit('/',1)[1][:-1] if x.endswith('>') else x).add_prefix('right_')\n",
    "\n",
    "#train\n",
    "wdc_headphones_train = ids_wdc_headphones_train.merge(\n",
    "    headphones_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    headphones_catalog_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "wdc_headphones_train['id'] = wdc_headphones_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#test\n",
    "wdc_headphones_test = ids_wdc_headphones_test.merge(\n",
    "    headphones_original,left_on=\"source_id\",right_on=\"left_subject_id\").merge(\n",
    "    headphones_catalog_original,left_on=\"target_id\",right_on=\"right_subject_id\").drop(['left_subject_id','right_subject_id'],1)\n",
    "wdc_headphones_test['id'] = wdc_headphones_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "\n",
    "#train\n",
    "gs_wdc_headphones_train['id'] = gs_wdc_headphones_train[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_wdc_headphones_train.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "#test\n",
    "gs_wdc_headphones_test['id'] = gs_wdc_headphones_test[['source_id','target_id']].astype(str).agg('-'.join,axis=1)\n",
    "gs_wdc_headphones_test.drop(['source_id','target_id'],axis=1,inplace=True)\n",
    "\n",
    "wdc_headphones_train = wdc_headphones_train.merge(gs_wdc_headphones_train,on='id')\n",
    "wdc_headphones_train = wdc_headphones_train.rename(columns={'matching':'label'})\n",
    "wdc_headphones_train.drop(\n",
    "    ['left_warc',\n",
    "    'left_url',\n",
    "    'right_mdoel',\n",
    "    'right_aditional_features',\n",
    "    'right_depth',\n",
    "    'right_sound_output_mode',\n",
    "    'right_detachable_cable',\n",
    "    'right_foldable',\n",
    "    'right_controls',\n",
    "    'right_microphone_response',\n",
    "    'right_microphone_audio_details',\n",
    "    'right_diaphragm',\n",
    "    'right_compliant_standards'],axis=1,inplace=True)\n",
    "wdc_headphones_train['label'] = wdc_headphones_train['label'].astype(int)\n",
    "\n",
    "wdc_headphones_test = wdc_headphones_test.merge(gs_wdc_headphones_test,on='id')\n",
    "wdc_headphones_test = wdc_headphones_test.rename(columns={'matching':'label'})\n",
    "wdc_headphones_test.drop(\n",
    "    ['left_warc',\n",
    "    'left_url',\n",
    "    'right_mdoel',\n",
    "    'right_aditional_features',\n",
    "    'right_depth',\n",
    "    'right_sound_output_mode',\n",
    "    'right_detachable_cable',\n",
    "    'right_foldable',\n",
    "    'right_controls',\n",
    "    'right_microphone_response',\n",
    "    'right_microphone_audio_details',\n",
    "    'right_diaphragm',\n",
    "    'right_compliant_standards'],axis=1,inplace=True)\n",
    "wdc_headphones_test['label'] = wdc_headphones_test['label'].astype(int)\n",
    "\n",
    "y = wdc_headphones_train['label']\n",
    "X_train, X_val, y_train, y_val = train_test_split(wdc_headphones_train, y, stratify=y, test_size=0.25)\n",
    " \n",
    "# Split Train into Train and Validation\n",
    "wdc_headphones_train = X_train\n",
    "wdc_headphones_validation = X_val\n",
    "wdc_headphones_train.to_csv(\"wdc_product/wdc_headphones_train\",index=False)\n",
    "wdc_headphones_validation.to_csv(\"wdc_product/wdc_headphones_validation\",index=False)\n",
    "wdc_headphones_test.to_csv(\"wdc_product/wdc_headphones_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "135\n737\n45\n246\n45\n245\n"
     ]
    }
   ],
   "source": [
    "print(sum(wdc_headphones_train['label'] == 1))\n",
    "print(sum(wdc_headphones_train['label'] == 0))\n",
    "print(sum(wdc_headphones_validation['label'] == 1))\n",
    "print(sum(wdc_headphones_validation['label'] == 0))\n",
    "print(sum(wdc_headphones_test['label'] == 1))\n",
    "print(sum(wdc_headphones_test['label'] == 0))"
   ]
  },
  {
   "source": [
    "# Import Computers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "computers_train = pd.read_json('computers/computers_train_small.json.gz', compression='gzip', lines=True)\n",
    "computers_gs = pd.read_json('computers/computers_gs.json.gz', compression='gzip', lines=True)\n",
    "computers_validation_ids = pd.read_csv('computers/computers_valid_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to relevant attributes and rename columns\n",
    "computers_train = computers_train[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into train into train and validation\n",
    "computers_validation = computers_train[computers_train['id'].isin(computers_validation_ids['pair_id'])]\n",
    "computers_train = computers_train[~computers_train['id'].isin(computers_validation_ids['pair_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess gs (test-set)\n",
    "computers_gs = computers_gs[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train, validation and test to csv\n",
    "computers_train.to_csv(\"computers/computers_train\",index=False)\n",
    "computers_validation.to_csv(\"computers/computers_validation\",index=False)\n",
    "computers_gs.to_csv(\"computers/computers_test\",index=False)"
   ]
  },
  {
   "source": [
    "# Import Cameras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras_train = pd.read_json('cameras/cameras_train_medium.json.gz', compression='gzip', lines=True)\n",
    "cameras_gs = pd.read_json('cameras/cameras_gs.json.gz', compression='gzip', lines=True)\n",
    "cameras_validation_ids = pd.read_csv('cameras/cameras_valid_medium.csv')\n",
    "\n",
    "# reduce to relevant attributes and rename columns\n",
    "cameras_train = cameras_train[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})\n",
    "\n",
    "# split train into train into train and validation\n",
    "cameras_validation = cameras_train[cameras_train['id'].isin(cameras_validation_ids['pair_id'])]\n",
    "cameras_train = cameras_train[~cameras_train['id'].isin(cameras_validation_ids['pair_id'])]\n",
    "\n",
    "# preprocess gs (test-set)\n",
    "cameras_gs = cameras_gs[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})\n",
    "\n",
    "# save train, validation and test to csv\n",
    "cameras_train.to_csv(\"cameras/cameras_train\",index=False)\n",
    "cameras_validation.to_csv(\"cameras/cameras_validation\",index=False)\n",
    "cameras_gs.to_csv(\"cameras/cameras_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "886\n3318\n222\n829\n300\n800\n"
     ]
    }
   ],
   "source": [
    "print(sum(pd.read_csv(\"cameras/cameras_train\")['label'] == 1))\n",
    "print(sum(pd.read_csv(\"cameras/cameras_train\")['label'] == 0))\n",
    "print(sum(pd.read_csv(\"cameras/cameras_validation\")['label'] == 1))\n",
    "print(sum(pd.read_csv(\"cameras/cameras_validation\")['label'] == 0))\n",
    "print(sum(pd.read_csv(\"cameras/cameras_test\")['label'] == 1))\n",
    "print(sum(pd.read_csv(\"cameras/cameras_test\")['label'] == 0))"
   ]
  },
  {
   "source": [
    "# Import Watches"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "watches_train = pd.read_json('watches/watches_train_large.json.gz', compression='gzip', lines=True)\n",
    "watches_gs = pd.read_json('watches/watches_gs.json.gz', compression='gzip', lines=True)\n",
    "watches_validation_ids = pd.read_csv('watches/watches_valid_large.csv')\n",
    "\n",
    "# reduce to relevant attributes and rename columns\n",
    "watches_train = watches_train[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})\n",
    "\n",
    "# split train into train into train and validation\n",
    "watches_validation = watches_train[watches_train['id'].isin(watches_validation_ids['pair_id'])]\n",
    "watches_train = watches_train[~watches_train['id'].isin(watches_validation_ids['pair_id'])]\n",
    "\n",
    "# preprocess gs (test-set)\n",
    "watches_gs = watches_gs[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})\n",
    "\n",
    "# save train, validation and test to csv\n",
    "watches_train.to_csv(\"watches/watches_train\",index=False)\n",
    "watches_validation.to_csv(\"watches/watches_validation\",index=False)\n",
    "watches_gs.to_csv(\"watches/watches_test\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4130\n",
      "17491\n",
      "1033\n",
      "4373\n",
      "300\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "print(sum(pd.read_csv(\"watches/watches_train\")['label'] == 1))\n",
    "print(sum(pd.read_csv(\"watches/watches_train\")['label'] == 0))\n",
    "print(sum(pd.read_csv(\"watches/watches_validation\")['label'] == 1))\n",
    "print(sum(pd.read_csv(\"watches/watches_validation\")['label'] == 0))\n",
    "print(sum(pd.read_csv(\"watches/watches_test\")['label'] == 1))\n",
    "print(sum(pd.read_csv(\"watches/watches_test\")['label'] == 0))"
   ]
  },
  {
   "source": [
    "# Import Shoes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoes_train = pd.read_json('shoes/shoes_train_xlarge.json.gz', compression='gzip', lines=True)\n",
    "shoes_gs = pd.read_json('shoes/shoes_gs.json.gz', compression='gzip', lines=True)\n",
    "shoes_validation_ids = pd.read_csv('shoes/shoes_valid_xlarge.csv')\n",
    "\n",
    "# reduce to relevant attributes and rename columns\n",
    "shoes_train = shoes_train[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})\n",
    "\n",
    "# split train into train into train and validation\n",
    "shoes_validation = shoes_train[shoes_train['id'].isin(shoes_validation_ids['pair_id'])]\n",
    "shoes_train = shoes_train[~shoes_train['id'].isin(shoes_validation_ids['pair_id'])]\n",
    "\n",
    "# preprocess gs (test-set)\n",
    "shoes_gs = shoes_gs[['pair_id','title_left','title_right','label']].rename(columns={'pair_id':'id','title_left':'left_title','title_right':'right_title'})\n",
    "\n",
    "# save train, validation and test to csv\n",
    "shoes_train.to_csv(\"shoes/shoes_train\",index=False)\n",
    "shoes_validation.to_csv(\"shoes/shoes_validation\",index=False)\n",
    "shoes_gs.to_csv(\"shoes/shoes_test\",index=False)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('python38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "3d6ccd2bcfab28fb4b08948d903be8a3077fc288a2616d4055f0b5a296f9cf65"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}